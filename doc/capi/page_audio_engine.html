<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Steam Audio API: Audio Engine Integration</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Steam Audio API
   &#160;<span id="projectnumber">2.0-beta.18</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',false,false,'search.php','Search');
});
</script>
<div id="main-nav"></div>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">index</a></li><li class="navelem"><a class="el" href="page_integration.html">Integration Guide</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Audio Engine Integration </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#audioengine_data">Representing Audio Data</a><ul><li class="level2"><a href="#audioengine_data_format">Supported Audio Formats</a></li>
<li class="level2"><a href="#audioengine_data_buffer">Audio Buffers</a></li>
</ul>
</li>
<li class="level1"><a href="#audioengine_direct">Direct Sound</a><ul><li class="level2"><a href="#audioengine_direct_hrtf">Spatializing Sound Sources</a></li>
<li class="level2"><a href="#audioengine_direct_attenuation">Attenuation and Occlusion</a></li>
</ul>
</li>
<li class="level1"><a href="#audioengine_indirect">Indirect Sound</a></li>
<li class="level1"><a href="#audioengine_spatialize">Spatializing Surround and Ambisonics Tracks</a></li>
</ul>
</div>
<div class="textblock"><p>This page provides an overview of the different Steam Audio components that can be used by an audio engine integration to apply the various Steam Audio effects.</p>
<p>For more details on how each of these features fit with the audio engine's overall signal flow, see <a class="el" href="page_audio_pipeline.html">Steam Audio Pipeline</a>.</p>
<h1><a class="anchor" id="audioengine_data"></a>
Representing Audio Data</h1>
<p>Digital audio systems represent sound using a series of <em>samples</em>. A sample measures sound intensity at a specific moment in time. A complete sound signal is stored by recording samples at regular intervals. The number of samples needed to represent 1 second of audio is referred to as the <em>sampling rate</em>; typical sampling rates are 44.1 kHz (44100 samples per second), 48 kHz, and so on.</p>
<h2><a class="anchor" id="audioengine_data_format"></a>
Supported Audio Formats</h2>
<p>Audio engines may process data in a variety of formats. Audio data can have multiple <em>channels</em>, for example, stereo (2 channels) and 5.1 surround (6 channels). Each sample comprises of one <em>element</em> for each channel (see the figure below). All elements for a single sample may be stored consecutively, followed by all elements for the next sample, and so on. This data layout is called <em>interleaved</em> storage. Alternatively, all elements for a single channel may be stored consecutively, followed by elements for the next channel, and so on. This data layout is called <em>deinterleaved</em> storage. Steam Audio lets you specify the format of any data passed to or from the audio engine; see <a class="el" href="struct_i_p_l_audio_format.html">IPLAudioFormat</a> for details.</p>
<p>In Steam Audio, all elements are represented using 32-bit, single-precision IEEE 754 floating-point numbers. Some audio engines may represent elements in a different format (for example, 16-bit signed integers). It is the application's responsibility to ensure that any necessary conversion is carried out before sending data to Steam Audio.</p>
<h2><a class="anchor" id="audioengine_data_buffer"></a>
Audio Buffers</h2>
<p>Most audio engines perform audio processing in <em>frames</em>. A frame is a fixed number of samples that are processed together. The parameters of any DSP algorithm do not change while processing a single frame; they may only be changed between frames. Typical frame sizes are 1024 or 512 samples. These audio frames are not related to the visual frames encountered in graphics. The audio engine is not tied to the graphical frame rate in any way.</p>
<p>You can pass frames of audio data to and from Steam Audio using <a class="el" href="struct_i_p_l_audio_buffer.html">IPLAudioBuffer</a>. For details on specifying audio engine sampling rates and frame sizes, see <a class="el" href="struct_i_p_l_rendering_settings.html">IPLRenderingSettings</a>.</p>
<h1><a class="anchor" id="audioengine_direct"></a>
Direct Sound</h1>
<p>Broadly, the two kinds of effects that can be applied to a sound to model the direct path from the source to listener are spatialization (HRTF) and non-spatialization effects (attenuation, occlusion).</p>
<h2><a class="anchor" id="audioengine_direct_hrtf"></a>
Spatializing Sound Sources</h2>
<p>Before HRTF-based binaural rendering can be applied to any sound source, a <a class="el" href="group__binauralrenderer.html">Binaural Renderer</a> object must be created using <a class="el" href="group__binauralrenderer.html#ga214f23f4a3952891a0881279083a370a">iplCreateBinauralRenderer</a>. There must only be a single Binaural Renderer object; it will be shared by all sources that will be rendered using HRTFs. The lifetime of a Binaural Renderer object must be at least as long as the total lifetime of all binaurally-rendered sources. When a Binaural Renderer object is no longer needed, destroy it using <a class="el" href="group__binauralrenderer.html#ga09c91f4334dd31236a54e352353d6c09">iplDestroyBinauralRenderer</a>.</p>
<p>For every source to which you want to apply binaural rendering, you must create an <a class="el" href="group__binauraleffect.html">Object-Based Binaural Effect</a> object, using <a class="el" href="group__binauraleffect.html#ga4a9fdcacc0818d11b631271e2db6cb4c">iplCreateBinauralEffect</a>. The lifetime of the Object-Based Binaural Effect object must be tied to the lifetime of the source itself. When the source is to be destroyed, destroy the Object-Based Binaural Effect object using <a class="el" href="group__binauraleffect.html#gab224f46a28b32933b3cfa0d7d398b540">iplDestroyBinauralEffect</a>. Every audio frame, use <a class="el" href="group__binauraleffect.html#gac5a700d7bc764d0d12553e07f6d7beaf">iplApplyBinauralEffect</a> to apply HRTF-based binaural rendering to the audio flowing through the source's audio graph.</p>
<h2><a class="anchor" id="audioengine_direct_attenuation"></a>
Attenuation and Occlusion</h2>
<p>You can use <a class="el" href="group__directsound.html#ga56c3aa3d888776ffdd583e8be4cd8229">iplGetDirectSoundPath</a> to query distance attenuation and occlusion parameters for a sound source. There are multiple occlusion algorithms you can use, see <a class="el" href="group__directsound.html#ga5ad1c145a41e584f2a01fac9f1af2a9a">IPLDirectOcclusionMethod</a> and <a class="el" href="group__directsound.html#ga6338a60ba7dea3f67f3578dad2182983">IPLDirectOcclusionMode</a> for details.</p>
<p>Before you can query occlusion parameters, you will need an <a class="el" href="group__envrenderer.html">Environmental Renderer</a> object. See below for details.</p>
<h1><a class="anchor" id="audioengine_indirect"></a>
Indirect Sound</h1>
<p>Indirect sound includes sound propagation effects applied to individual sources, as well as physics-based reverb effects applied to mixed buffers of audio.</p>
<p>Before indirect sound effects can be applied, you must create an <a class="el" href="group__envrenderer.html">Environmental Renderer</a> object, using <a class="el" href="group__envrenderer.html#gaf39736c9b9d6bf5ed191fc667a8251aa">iplCreateEnvironmentalRenderer</a>. Creating this object requires that the game engine specify an <a class="el" href="group__environment.html">Environment</a> object; see <a class="el" href="page_game_engine.html">Game Engine Integration</a> for details on how to do this. Only one Environmental Renderer object may exist at any point in time. It will be shared by all sound sources and buses that need to apply indirect sound effects. When you no longer need an Environmental Renderer object, you can destroy it using <a class="el" href="group__envrenderer.html#ga25058ad5cce9c333d72bcb38a13b9d32">iplDestroyEnvironmentalRenderer</a>.</p>
<p>For every source (or bus) to which you want to apply sound propagation (or reverb) effects, you must create a <a class="el" href="group__conveffect.html">Convolution Effect</a> object, using <a class="el" href="group__conveffect.html#ga9cfbfbac22e6c7688104b22ac4164576">iplCreateConvolutionEffect</a>. The lifetime of the Convolution Effect object must be tied to the lifetime of the source itself. When the source is to be destroyed, the Convolution Effect object can be destroyed using <a class="el" href="group__conveffect.html#ga8f98006808fcddfca853fe14b04971e6">iplDestroyConvolutionEffect</a>. Every audio frame, using <a class="el" href="group__conveffect.html#gafaeedf252420be0e6e29d512ba2b05ff">iplSetDryAudioForConvolutionEffect</a> and <a class="el" href="group__conveffect.html#gadab85ed82af4f62a481c2ac7476da565">iplGetWetAudioForConvolutionEffect</a> to apply indirect sound effects to the incoming audio data.</p>
<p>For increased performance, you can use <a class="el" href="group__conveffect.html#gae003bef1c43dd9253add6d4ae22dec2c">iplGetMixedEnvironmentalAudio</a> instead of <a class="el" href="group__conveffect.html#gadab85ed82af4f62a481c2ac7476da565">iplGetWetAudioForConvolutionEffect</a>. This function must be called at a point in the audio graph where indirect sound is to be mixed. The output of this function is the same as calling <a class="el" href="group__conveffect.html#gadab85ed82af4f62a481c2ac7476da565">iplGetWetAudioForConvolutionEffect</a> on all Convolution Effect objects that have been created, and summing the results. Note that this approach prevents additional user-defined effects from being inserted after the indirect sound effects applied via the Convolution Effect object.</p>
<h1><a class="anchor" id="audioengine_spatialize"></a>
Spatializing Surround and Ambisonics Tracks</h1>
<p>You can use Steam Audio to spatialize surround or Ambisonics audio flowing through any point in a bus or submix voice.</p>
<p>Surround audio (quadraphonic, 5.1, or 7.1) can be rendered using HRTFs using a <a class="el" href="group__virtualsurround.html">Virtual Surround Effect</a> object. Create the Virtual Surround Effect object when the bus or submix voice is created, using <a class="el" href="group__virtualsurround.html#ga863ae440ab8c77eb47221ca73acfb981">iplCreateVirtualSurroundEffect</a>. When the bus or submix voice is destroyed, destroy the Virtual Surround Effect object using <a class="el" href="group__virtualsurround.html#ga6654a7a11fa8b4fdcf096694f3f39656">iplDestroyVirtualSurroundEffect</a>. Every audio frame, call <a class="el" href="group__virtualsurround.html#ga0a7beb1a7f78534a2b1addfa58db811b">iplApplyVirtualSurroundEffect</a> to apply HRTF-based binaural rendering to the surround audio data.</p>
<p>Ambisonics audio can be rendered using HRTFs (or decoded to a surround format) using an <a class="el" href="group__ambisonics.html">Ambisonics Binaural Effect</a> (or <a class="el" href="group__ambisonicspanning.html">Ambisonics Panning Effect</a>) object. Create the Ambisonics Binaural Effect (or Ambisonics Panning Effect) object when the bus or submix voice is created, using <a class="el" href="group__ambisonics.html#ga777ab70f2b585d8af92e23c08db01c5e">iplCreateAmbisonicsBinauralEffect</a> (or <a class="el" href="group__ambisonicspanning.html#ga26191be4b6db20b5fc3a8103d0984156">iplCreateAmbisonicsPanningEffect</a>). When the bus or submix voice is destroyed, destroy the Ambisonics Binaural Effect (or Ambisonics Panning Effect) object using <a class="el" href="group__ambisonics.html#ga7407738ab9852fe98aaed3b299a2a44c">iplDestroyAmbisonicsBinauralEffect</a> (or <a class="el" href="group__ambisonicspanning.html#gac393a7fbd002a5c946a5a51a179b3317">iplDestroyAmbisonicsPanningEffect</a>). Every audio frame, call <a class="el" href="group__ambisonics.html#ga3cd19e5560e9c03b51d789d4fb4bc29c">iplApplyAmbisonicsBinauralEffect</a> (or <a class="el" href="group__ambisonicspanning.html#gae9c5ca6ae1219ae575a6eb1d1a20b485">iplApplyAmbisonicsPanningEffect</a>) to apply HRTF-based binaural rendering (or Ambisonics-to-surround decoding) to the Ambisonics audio data. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
